{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this awesome data science series, we have looked at my playground where we loaded and analyzed the [Netcla dataset](http://www.neteye-blog.com/netcla-the-ecml-pkdd-network-classification-challenge/). We trained a couple of basic prediction models, then we also trained a few ensemble models.\n",
    "\n",
    "In order to save our precious time, we always trained only on a sample of the whole dataset. That's probably why the scores were pretty low for most models. We also lacked the domain knowledge to fully understand the data. Therefore, preprocessing was only basic and it further lowered the scores of the models.\n",
    "\n",
    "The last thing we shall do is to load our saved models, predict labels for the validation data and run the `eval.py` script to see our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import process\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, _ = process.load_dataset_target('data/valid.csv', 'data/valid_target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "These are my personal notes for this task. I must say, it was a lot of fun! I enjoyed writing Python code, transforming the data, and trying various predictors and seeing how they perform. I lacked domain knowledge and therefore didn't bother much with feature engineering and looking into data properly. For instance, I didn't care about outliers.\n",
    "\n",
    "When I started, I wrote 2 transforms, one of which is the `BoxcoxTransform` (in file `scripts/transforms.py`) which applies boxcox tranform to the dataset. Unfortunately, I wrote it bad, it performed boxcox transforms for each row, instead for each column. These also involved a huge performance penalty.\n",
    "\n",
    "Of course, I didn't know, so I always used only a small subset of the whole dataset for training models. After I fixed `BoxcoxTransform`, suddenly things moved very quickly. For example, the extra trees model originally trained for 25 minutes using a small subset of the dataset. After the fix, it only trained for 16 seconds!\n",
    "\n",
    "Amazed by this new found POWER, I trained on the whole dataset using more parameters for grid search and more complex models. Even more fun!\n",
    "\n",
    "Finally, thank you for this class. It was AMAZING!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra trees with grid search\n",
    "\n",
    "First, predict and store the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   7 out of  10 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "trees_model = joblib.load('models/extratrees.pkl')\n",
    "trees_result = pd.DataFrame(trees_model.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees_result.to_csv('predictions/extratrees.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating predictions/extratrees.csv using as gold labels data/valid_target.csv\n",
      "\n",
      "Detailed Report:\n",
      "  category  ||   precision  ||     recall   ||       F1     ||    examples  ||\n",
      "=============================================================================\n",
      "1           ||        0.0000||        0.0000||        0.0000||           644||\n",
      "4           ||        0.0000||        0.0000||        0.0000||          6736||\n",
      "5           ||        0.0000||        0.0000||        0.0000||          8104||\n",
      "6           ||        0.0000||        0.0000||        0.0000||         13764||\n",
      "8           ||        0.0000||        0.0000||        0.0000||         40961||\n",
      "12          ||        0.0000||        0.0000||        0.0000||         31985||\n",
      "13          ||        0.0000||        0.0000||        0.0000||          2807||\n",
      "14          ||        0.0000||        0.0000||        0.0000||         22590||\n",
      "15          ||        0.0000||        0.0000||        0.0000||           781||\n",
      "19          ||        0.0000||        0.0000||        0.0000||          5482||\n",
      "20          ||        0.0000||        0.0000||        0.0000||          2013||\n",
      "23          ||        0.0000||        0.0000||        0.0000||           188||\n",
      "25          ||        0.0000||        0.0000||        0.0000||           352||\n",
      "27          ||        0.0000||        0.0000||        0.0000||           662||\n",
      "29          ||        0.0000||        0.0000||        0.0000||           235||\n",
      "34          ||        0.0000||        0.0000||        0.0000||           206||\n",
      "35          ||        0.0000||        0.0000||        0.0000||           527||\n",
      "36          ||        0.0000||        0.0000||        0.0000||          4885||\n",
      "42          ||        0.0000||        0.0000||        0.0000||          2831||\n",
      "=============================================================================\n",
      "Macro       ||        0.0000||        0.0000||        0.0000||        145753||\n",
      "Micro       ||        0.0000||        0.0000||        0.0000||        145753||\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 scripts/eval.py data/valid_target.csv predictions/extratrees.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "First, predict and store the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator VarianceThreshold from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator GaussianNB from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator BaggingClassifier from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator Pipeline from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator VotingClassifier from version 0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/site-packages/scipy/stats/morestats.py:905: RuntimeWarning: overflow encountered in square\n",
      "  llf -= N / 2.0 * np.log(np.sum((y - y_mean)**2. / N, axis=0))\n",
      "/usr/local/lib/python3.5/site-packages/scipy/optimize/optimize.py:1876: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:  1.1min remaining:  1.8min\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   33.7s remaining:   56.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "ensemble_model = joblib.load('models/ensemble.pkl')\n",
    "ensemble_result = pd.DataFrame(ensemble_model.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_result.to_csv('predictions/ensemble.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating predictions/ensemble.csv using as gold labels data/valid_target.csv\n",
      "\n",
      "Detailed Report:\n",
      "  category  ||   precision  ||     recall   ||       F1     ||    examples  ||\n",
      "=============================================================================\n",
      "1           ||        0.0023||        0.6925||        0.0045||           644||\n",
      "4           ||        0.0263||        0.8481||        0.0510||          6736||\n",
      "5           ||        0.0000||        0.0000||        0.0000||          8104||\n",
      "6           ||        0.3676||        0.4804||        0.4165||         13764||\n",
      "8           ||        0.4015||        0.0039||        0.0077||         40961||\n",
      "12          ||        0.9990||        0.2232||        0.3649||         31985||\n",
      "13          ||        0.0000||        0.0000||        0.0000||          2807||\n",
      "14          ||        0.0000||        0.0000||        0.0000||         22590||\n",
      "15          ||        0.0022||        0.5595||        0.0045||           781||\n",
      "19          ||        0.0289||        0.5252||        0.0548||          5482||\n",
      "20          ||        0.0000||        0.0000||        0.0000||          2013||\n",
      "23          ||        0.0000||        0.0000||        0.0000||           188||\n",
      "25          ||        0.0000||        0.0000||        0.0000||           352||\n",
      "27          ||        0.0000||        0.0000||        0.0000||           662||\n",
      "29          ||        0.0000||        0.0000||        0.0000||           235||\n",
      "34          ||        0.0000||        0.0000||        0.0000||           206||\n",
      "35          ||        0.0000||        0.0000||        0.0000||           527||\n",
      "36          ||        0.0000||        0.0000||        0.0000||          4885||\n",
      "42          ||        0.0000||        0.0000||        0.0000||          2831||\n",
      "=============================================================================\n",
      "Macro       ||        0.0962||        0.1754||        0.1243||        145753||\n",
      "Micro       ||        0.0319||        0.1604||        0.0532||        145753||\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 scripts/eval.py data/valid_target.csv predictions/ensemble.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "I don't really now why both models perform so poorly. The extra trees model even does not work, it always predicts 0.\n",
    "\n",
    "One possible reason for this is that the input data is not being transformed by the model before predicting.\n",
    "\n",
    "Anyway, I'm signing off. Have a good day!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
